# Web Crawler for API Reconnaissance & Anti-Bot Evasion

## ğŸ“Œ Overview
This project is part of a **Masterâ€™s Report in Cybersecurity & Information Operations** from the **University of Arizona**. It aims to develop a **Python-based web crawler** that progressively increases in complexity, from **basic web scraping** to **bypassing advanced anti-bot protections**.

The crawler aims to demonstrate:
- **Scraping static & dynamic web pages.**
- **Detecting API endpoints for reconnaissance & reverse engineering.**
- **Bypassing anti-bot defenses (Cloudflare, Akamai, Datadome).**
- **Utilizing Playwright, Puppeteer, Scrapy, Selenium, and BeautifulSoup.**

---

## ğŸš€ Setup Instructions
1. **Clone the repository:**
   ```bash
   git clone https://github.com/ChristianTStu/web_crawler_project.git
   ```
2. **Navigate into the project folder:**
   ```bash
   cd web_crawler_project
   ```
3. **Create and activate a virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows use: venv\Scripts\activate
   ```
4. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ›  Features & Methodology
This project is divided into **4 progressive phases**:

### **1ï¸âƒ£ Basic Crawler (Scrapy & BeautifulSoup)**
- Crawls and extracts **static web content** from simple pages.
- Stores structured data in **JSON/CSV format**.

### **2ï¸âƒ£ JavaScript-Based Crawler (Playwright & Selenium)**
- Handles **JavaScript-heavy websites** (AJAX, dynamic loading).
- Simulates **user interactions** in a real browser.

### **3ï¸âƒ£ API Endpoint Detection (Passive Reconnaissance)**
- Extracts **hidden API endpoints** from JavaScript files.
- Helps analyze **API security vulnerabilities**.

### **4ï¸âƒ£ Advanced Crawler (Playwright & Puppeteer)**
- Implements **browser fingerprint spoofing** & **headless detection bypass**.
- Uses **proxy rotation & CAPTCHA solving** for anti-bot evasion.

---

## ğŸ“‚ Project Structure
```plaintext
web_crawler_project/
â”‚â”€â”€ crawlers/             # Web crawler scripts
â”‚â”€â”€ logs/                 # Logs for debugging
â”‚â”€â”€ results/              # Scraped data
â”‚â”€â”€ tests/                # Unit tests
â”‚â”€â”€ utils/                # Helper functions
â”‚â”€â”€ venv/                 # Virtual environment (ignored by Git)
â”‚â”€â”€ .gitignore            # Files to ignore
â”‚â”€â”€ requirements.txt      # Dependencies list
â”‚â”€â”€ main.py               # CLI tool entry point
â”‚â”€â”€ README.md             # This file
```
---
## ğŸ”¹ Ethical & Legal Considerations
- **Testing is only conducted on legally permissible targets.**
- **Respects `robots.txt` policies where applicable.**
- **No unauthorized data extraction is performed.**
- **Compliance with data protection laws (GDPR, CCPA).**
